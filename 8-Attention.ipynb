{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c321da96",
   "metadata": {},
   "source": [
    "# 8.Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ad7748",
   "metadata": {},
   "source": [
    "## 8.1 Attention的结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b7b7332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 4)\n"
     ]
    }
   ],
   "source": [
    "# seq2seq存在的问题\n",
    "# 编码器的改进 1\n",
    "import numpy as np\n",
    "\n",
    "T, H = 5, 4\n",
    "hs = np.random.rand(T, H)\n",
    "a = np.array([0.8, 0.1, 0.03, 0.05, 0.02])\n",
    "ar = a.reshape(5, 1).repeat(4, axis=1)\n",
    "print(ar.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fbbd4f95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.69247824, 0.76194075, 0.78304773, 0.47487055])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(hs * ar, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "45078b7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 5, 4)\n",
      "(10, 4)\n"
     ]
    }
   ],
   "source": [
    "N, T, H = 10, 5, 4\n",
    "hs = np.random.randn(N, T, H)\n",
    "a = np.random.rand(N, T)\n",
    "ar = a.reshape(N, T, 1).repeat(H, axis=2)\n",
    "\n",
    "t = hs * ar\n",
    "print(t.shape)\n",
    "\n",
    "c = np.sum(t, axis=1)\n",
    "print(c.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "109dae6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightSum:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.cache = None\n",
    "        \n",
    "    def forward(self, hs, a):\n",
    "        N, T, H = hs.shape\n",
    "        ar = a.reshape(N, T, 1).repeat(H, axis = 2)\n",
    "        t = hs * ar\n",
    "        c = np.sum(t, axis = 1)\n",
    "        self.cache = (hs, ar)\n",
    "        return c\n",
    "    \n",
    "    def backward(self, dc):\n",
    "        hs, ar = self.cache\n",
    "        N, T, H = hs.shape\n",
    "        dt = dc.reshape(N, 1, H).repeat(T, axis = 1)\n",
    "        dar = dt * hs\n",
    "        dhs = dt * ar\n",
    "        da = np.sum(dar, axis = 2)\n",
    "        return dhs, da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "37e760bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 5, 4)\n",
      "(10, 5)\n",
      "(10, 5)\n"
     ]
    }
   ],
   "source": [
    "# 编码器的改进 2\n",
    "from common.layers import Softmax\n",
    "import numpy as np\n",
    "\n",
    "N, T, H = 10, 5, 4\n",
    "hs = np.random.randn(N, T, H)\n",
    "h = np.random.randn(N, H)\n",
    "hr = h.reshape(N, 1, H).repeat(T, axis=1)\n",
    "\n",
    "t = hs * hr\n",
    "print(t.shape)\n",
    "\n",
    "s = np.sum(t, axis=2)\n",
    "print(s.shape)\n",
    "\n",
    "softmax = Softmax()\n",
    "a = softmax.forward(s)\n",
    "print(a.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "249b156f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.layers import Softmax\n",
    "import numpy as np\n",
    "\n",
    "class AttentionWeight:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.softmax = Softmax()\n",
    "        self.cache = None\n",
    "        \n",
    "    def forward(self, hs, h):\n",
    "        N, T, H = hs.shape\n",
    "        hr = h.reshape(N, 1, H).repeat(T, axis=1)\n",
    "        t = hs * hr\n",
    "        s = np.sum(t, axis=2)\n",
    "        a = self.softmax.forward(s)\n",
    "        \n",
    "        self.cache = (hs, hr)\n",
    "        return a\n",
    "    \n",
    "    def backward(self, da):\n",
    "        hs, hr = self.cache\n",
    "        N, T, H = hs.shape\n",
    "        \n",
    "        ds = self.softmax.backward(da)\n",
    "        dt = ds.reshape(N, T, 1).repeat(H, axis=2)\n",
    "        dhs = dt * hr\n",
    "        dhr = dt * hs\n",
    "        dh = np.sum(dhr, axis=1)\n",
    "        return dhs, dh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ffdfe97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.attention_weight_layer = AttentionWeight()\n",
    "        self.weight_sum_layer = WeightSum()\n",
    "        self.attention_weight = None\n",
    "        \n",
    "    def forward(self, hs, h):\n",
    "        a = self.attention_weight_layer.forward(hs, h)\n",
    "        out = self.weight_sum_layer.forward(hs, a)\n",
    "        self.attention_weight = a\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dhs0, da = self.weight_sum_layer.backward(dout)\n",
    "        dhs1, dh = self.attention_weight_layer.backward(da)\n",
    "        dhs = dhs0 + dhs1\n",
    "        return dhs, dh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1ec9b42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeAttention:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.layers = None\n",
    "        self.attention_weights = None\n",
    "        \n",
    "    def forward(self, hs_enc, hs_dec):\n",
    "        N, T, H = hs_dec.shape\n",
    "        out = np.empty_like(hs_dec)\n",
    "        self.layers = []\n",
    "        self.attention_weights = []\n",
    "        \n",
    "        for t in range(T):\n",
    "            layer = Attention()\n",
    "            out[:, t, :] = layer.forward(hs_enc, hs_dec[:, t, :])\n",
    "            self.layers.append(layer)\n",
    "            self.attention_weights.append(layer.attention_weight)\n",
    "            \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        N, T, H = dout.shape\n",
    "        dhs_enc = 0\n",
    "        dhs_dec = np.empty_like(dout)\n",
    "        \n",
    "        for t in range(T):\n",
    "            layer = self.layers[t]\n",
    "            dhs, dh = layer.backward(dout[:, t, :])\n",
    "            dhs_enc += dhs\n",
    "            dhs_dec[:, t, :] = dh\n",
    "            \n",
    "        return dhs_enc, dhs_dec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db162033",
   "metadata": {},
   "source": [
    "## 8.2 带attention的seq2seq实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c5a5707",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 编码器的实现\n",
    "from common.time_layers import *\n",
    "from ch07.seq2seq import Encoder, Seq2seq\n",
    "from ch08.attention_layer import TimeAttention\n",
    "\n",
    "class AttentionEncoder(Encoder):\n",
    "    def forward(self, xs):\n",
    "        xs = self.embed.forward(xs)\n",
    "        hs = self.lstm.forward(xs)\n",
    "        return hs\n",
    "    \n",
    "    def backward(self, dhs):\n",
    "        dout = self.lstm.backward(dhs)\n",
    "        dout = self.embed.backward(dout)\n",
    "        return dout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69a42256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解码器的实现\n",
    "class AttentionDecoder:\n",
    "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
    "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
    "        rn = np.random.randn\n",
    "        \n",
    "        embed_W = (rn(V, D) / 100).astype('f')\n",
    "        lstm_Wx = (rn(D, 4 * H) / np.sqrt(D)).astype('f')\n",
    "        lstm_Wh = (rn(H, 4 * H)) / np.sqrt(H).astype('f')\n",
    "        lstm_b = np.zeros(4 * H).astype('f')\n",
    "        affine_W = (rn(2*H, V)) / np.sqrt(2*H).astype('f')\n",
    "        affine_b = np.zeros(V).astype('f')\n",
    "        \n",
    "        self.embed = TimeEmbedding(embed_W)\n",
    "        self.lstm = TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful=True)\n",
    "        self.attention = TimeAttention()\n",
    "        self.affine = TimeAffine(affine_W, affine_b)\n",
    "        layers = [self.embed, self.lstm, self.attention, self.affine]\n",
    "        \n",
    "        self.params, self.grads = [], []\n",
    "        for layer in layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "            \n",
    "    def forward(self, sx, enc_hs):\n",
    "        h = enc_hs[:, -1]\n",
    "        self.lstm.set_state(h)\n",
    "\n",
    "        out = self.embed.forward(xs)\n",
    "        dec_hs = self.lstm.forward(out)\n",
    "        c = self.attention.forward(enc_hs, dec_hs)\n",
    "        out = np.concatenate((c, dec_hs), axis=2)\n",
    "        score = self.affine.forward(out)\n",
    "\n",
    "        return score\n",
    "\n",
    "    def backward(self, dscore):\n",
    "        dout = self.affine.backward(dscore)\n",
    "        N, T, H2 = dout.shape\n",
    "        H = H2 // 2\n",
    "\n",
    "        dc, ddec_hs0 = dout[:,:,:H], dout[:,:,H:]\n",
    "        denc_hs, ddec_hs1 = self.attention.backward(dc)\n",
    "        ddec_hs = ddec_hs0 + ddec_hs1\n",
    "        dout = self.lstm.backward(ddec_hs)\n",
    "        dh = self.lstm.dh\n",
    "        denc_hs[:, -1] += dh\n",
    "        self.embed.backward(dout)\n",
    "\n",
    "        return denc_hs\n",
    "    \n",
    "    def generate(self, enc_hs, start_id, sample_size):\n",
    "        sampled = []\n",
    "        sample_id = start_id\n",
    "        h = enc_hs[:, -1]\n",
    "        self.lstm.set_state(h)\n",
    "\n",
    "        for _ in range(sample_size):\n",
    "            x = np.array([sample_id]).reshape((1, 1))\n",
    "\n",
    "            out = self.embed.forward(x)\n",
    "            dec_hs = self.lstm.forward(out)\n",
    "            c = self.attention.forward(enc_hs, dec_hs)\n",
    "            out = np.concatenate((c, dec_hs), axis=2)\n",
    "            score = self.affine.forward(out)\n",
    "\n",
    "            sample_id = np.argmax(score.flatten())\n",
    "            sampled.append(sample_id)\n",
    "\n",
    "        return sampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "290a5562",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 带attention的seq2seq的实现\n",
    "class AttentionSeq2seq(Seq2seq):\n",
    "    def __init_(self, vocab_size, wordvec_size, hidden_size):\n",
    "        args = vocab_size, wordvec_size, hidden_size\n",
    "        self.encoder = AttentionEncoder(*args)\n",
    "        self.decoder = AttentionDecoder(*args)\n",
    "        self.softmax = TimeSoftmaxWithLoss()\n",
    "\n",
    "        self.params = self.encoder.params + self.decoder.params\n",
    "        self.grads = self.encoder.grads + self.decoder.grads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e6fe4f",
   "metadata": {},
   "source": [
    "## 8.3 Attention的评价"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b6d27ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch 1 |  iter 1 / 351 | time 0[s] | loss 4.08\n",
      "| epoch 1 |  iter 21 / 351 | time 8[s] | loss 3.09\n",
      "| epoch 1 |  iter 41 / 351 | time 16[s] | loss 1.90\n",
      "| epoch 1 |  iter 61 / 351 | time 24[s] | loss 1.72\n",
      "| epoch 1 |  iter 81 / 351 | time 32[s] | loss 1.46\n",
      "| epoch 1 |  iter 101 / 351 | time 40[s] | loss 1.19\n",
      "| epoch 1 |  iter 121 / 351 | time 48[s] | loss 1.14\n",
      "| epoch 1 |  iter 141 / 351 | time 56[s] | loss 1.09\n",
      "| epoch 1 |  iter 161 / 351 | time 65[s] | loss 1.06\n",
      "| epoch 1 |  iter 181 / 351 | time 73[s] | loss 1.04\n",
      "| epoch 1 |  iter 201 / 351 | time 83[s] | loss 1.03\n",
      "| epoch 1 |  iter 221 / 351 | time 92[s] | loss 1.02\n",
      "| epoch 1 |  iter 241 / 351 | time 100[s] | loss 1.02\n",
      "| epoch 1 |  iter 261 / 351 | time 108[s] | loss 1.01\n",
      "| epoch 1 |  iter 281 / 351 | time 116[s] | loss 1.00\n",
      "| epoch 1 |  iter 301 / 351 | time 125[s] | loss 1.00\n",
      "| epoch 1 |  iter 321 / 351 | time 133[s] | loss 1.00\n",
      "| epoch 1 |  iter 341 / 351 | time 141[s] | loss 1.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "\u001b[91m☒\u001b[0m 1978-08-11\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "\u001b[91m☒\u001b[0m 1978-08-11\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "\u001b[91m☒\u001b[0m 1978-08-11\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "\u001b[91m☒\u001b[0m 1978-08-11\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "\u001b[91m☒\u001b[0m 1978-08-11\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "\u001b[91m☒\u001b[0m 1978-08-11\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "\u001b[91m☒\u001b[0m 1978-08-11\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "\u001b[91m☒\u001b[0m 1978-08-11\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "\u001b[91m☒\u001b[0m 1978-08-11\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "\u001b[91m☒\u001b[0m 1978-08-11\n",
      "---\n",
      "val acc 0.000%\n",
      "| epoch 2 |  iter 1 / 351 | time 0[s] | loss 1.00\n",
      "| epoch 2 |  iter 21 / 351 | time 8[s] | loss 1.00\n",
      "| epoch 2 |  iter 41 / 351 | time 16[s] | loss 0.99\n",
      "| epoch 2 |  iter 61 / 351 | time 24[s] | loss 0.99\n",
      "| epoch 2 |  iter 81 / 351 | time 32[s] | loss 0.99\n",
      "| epoch 2 |  iter 101 / 351 | time 41[s] | loss 0.99\n",
      "| epoch 2 |  iter 121 / 351 | time 48[s] | loss 0.99\n",
      "| epoch 2 |  iter 141 / 351 | time 56[s] | loss 0.98\n",
      "| epoch 2 |  iter 161 / 351 | time 64[s] | loss 0.98\n",
      "| epoch 2 |  iter 181 / 351 | time 72[s] | loss 0.97\n",
      "| epoch 2 |  iter 201 / 351 | time 80[s] | loss 0.95\n",
      "| epoch 2 |  iter 221 / 351 | time 88[s] | loss 0.94\n",
      "| epoch 2 |  iter 241 / 351 | time 96[s] | loss 0.90\n",
      "| epoch 2 |  iter 261 / 351 | time 104[s] | loss 0.83\n",
      "| epoch 2 |  iter 281 / 351 | time 112[s] | loss 0.74\n",
      "| epoch 2 |  iter 301 / 351 | time 120[s] | loss 0.66\n",
      "| epoch 2 |  iter 321 / 351 | time 128[s] | loss 0.58\n",
      "| epoch 2 |  iter 341 / 351 | time 136[s] | loss 0.46\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "\u001b[92m☑\u001b[0m 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "\u001b[91m☒\u001b[0m 2006-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "\u001b[92m☑\u001b[0m 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "\u001b[92m☑\u001b[0m 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "\u001b[92m☑\u001b[0m 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "\u001b[92m☑\u001b[0m 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "\u001b[92m☑\u001b[0m 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "\u001b[91m☒\u001b[0m 2007-08-09\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "\u001b[91m☒\u001b[0m 1983-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "\u001b[91m☒\u001b[0m 2016-11-08\n",
      "---\n",
      "val acc 51.560%\n",
      "| epoch 3 |  iter 1 / 351 | time 0[s] | loss 0.35\n",
      "| epoch 3 |  iter 21 / 351 | time 8[s] | loss 0.30\n",
      "| epoch 3 |  iter 41 / 351 | time 16[s] | loss 0.21\n",
      "| epoch 3 |  iter 61 / 351 | time 24[s] | loss 0.14\n",
      "| epoch 3 |  iter 81 / 351 | time 32[s] | loss 0.09\n",
      "| epoch 3 |  iter 101 / 351 | time 40[s] | loss 0.07\n",
      "| epoch 3 |  iter 121 / 351 | time 48[s] | loss 0.05\n",
      "| epoch 3 |  iter 141 / 351 | time 56[s] | loss 0.04\n",
      "| epoch 3 |  iter 161 / 351 | time 64[s] | loss 0.03\n",
      "| epoch 3 |  iter 181 / 351 | time 72[s] | loss 0.03\n",
      "| epoch 3 |  iter 201 / 351 | time 80[s] | loss 0.02\n",
      "| epoch 3 |  iter 221 / 351 | time 88[s] | loss 0.02\n",
      "| epoch 3 |  iter 241 / 351 | time 97[s] | loss 0.02\n",
      "| epoch 3 |  iter 261 / 351 | time 105[s] | loss 0.01\n",
      "| epoch 3 |  iter 281 / 351 | time 113[s] | loss 0.01\n",
      "| epoch 3 |  iter 301 / 351 | time 121[s] | loss 0.01\n",
      "| epoch 3 |  iter 321 / 351 | time 130[s] | loss 0.01\n",
      "| epoch 3 |  iter 341 / 351 | time 139[s] | loss 0.01\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "\u001b[92m☑\u001b[0m 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "\u001b[92m☑\u001b[0m 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "\u001b[92m☑\u001b[0m 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "\u001b[92m☑\u001b[0m 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "\u001b[92m☑\u001b[0m 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "\u001b[92m☑\u001b[0m 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "\u001b[92m☑\u001b[0m 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "\u001b[92m☑\u001b[0m 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "\u001b[92m☑\u001b[0m 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "\u001b[92m☑\u001b[0m 2016-11-06\n",
      "---\n",
      "val acc 99.900%\n",
      "| epoch 4 |  iter 1 / 351 | time 0[s] | loss 0.01\n",
      "| epoch 4 |  iter 21 / 351 | time 8[s] | loss 0.01\n",
      "| epoch 4 |  iter 41 / 351 | time 16[s] | loss 0.01\n",
      "| epoch 4 |  iter 61 / 351 | time 24[s] | loss 0.01\n",
      "| epoch 4 |  iter 81 / 351 | time 33[s] | loss 0.01\n",
      "| epoch 4 |  iter 101 / 351 | time 41[s] | loss 0.01\n",
      "| epoch 4 |  iter 121 / 351 | time 49[s] | loss 0.00\n",
      "| epoch 4 |  iter 141 / 351 | time 58[s] | loss 0.01\n",
      "| epoch 4 |  iter 161 / 351 | time 66[s] | loss 0.00\n",
      "| epoch 4 |  iter 181 / 351 | time 74[s] | loss 0.00\n",
      "| epoch 4 |  iter 201 / 351 | time 83[s] | loss 0.00\n",
      "| epoch 4 |  iter 221 / 351 | time 91[s] | loss 0.00\n",
      "| epoch 4 |  iter 241 / 351 | time 99[s] | loss 0.00\n",
      "| epoch 4 |  iter 261 / 351 | time 108[s] | loss 0.00\n",
      "| epoch 4 |  iter 281 / 351 | time 116[s] | loss 0.01\n",
      "| epoch 4 |  iter 301 / 351 | time 125[s] | loss 0.00\n",
      "| epoch 4 |  iter 321 / 351 | time 133[s] | loss 0.00\n",
      "| epoch 4 |  iter 341 / 351 | time 141[s] | loss 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "\u001b[92m☑\u001b[0m 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "\u001b[92m☑\u001b[0m 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "\u001b[92m☑\u001b[0m 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "\u001b[92m☑\u001b[0m 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "\u001b[92m☑\u001b[0m 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "\u001b[92m☑\u001b[0m 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "\u001b[92m☑\u001b[0m 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "\u001b[92m☑\u001b[0m 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "\u001b[92m☑\u001b[0m 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "\u001b[92m☑\u001b[0m 2016-11-06\n",
      "---\n",
      "val acc 99.900%\n",
      "| epoch 5 |  iter 1 / 351 | time 0[s] | loss 0.00\n",
      "| epoch 5 |  iter 21 / 351 | time 8[s] | loss 0.00\n",
      "| epoch 5 |  iter 41 / 351 | time 17[s] | loss 0.00\n",
      "| epoch 5 |  iter 61 / 351 | time 25[s] | loss 0.00\n",
      "| epoch 5 |  iter 81 / 351 | time 34[s] | loss 0.00\n",
      "| epoch 5 |  iter 101 / 351 | time 42[s] | loss 0.00\n",
      "| epoch 5 |  iter 121 / 351 | time 50[s] | loss 0.00\n",
      "| epoch 5 |  iter 141 / 351 | time 59[s] | loss 0.00\n",
      "| epoch 5 |  iter 161 / 351 | time 67[s] | loss 0.00\n",
      "| epoch 5 |  iter 181 / 351 | time 75[s] | loss 0.00\n",
      "| epoch 5 |  iter 201 / 351 | time 83[s] | loss 0.00\n",
      "| epoch 5 |  iter 221 / 351 | time 92[s] | loss 0.00\n",
      "| epoch 5 |  iter 241 / 351 | time 100[s] | loss 0.00\n",
      "| epoch 5 |  iter 261 / 351 | time 108[s] | loss 0.00\n",
      "| epoch 5 |  iter 281 / 351 | time 116[s] | loss 0.00\n",
      "| epoch 5 |  iter 301 / 351 | time 124[s] | loss 0.00\n",
      "| epoch 5 |  iter 321 / 351 | time 132[s] | loss 0.00\n",
      "| epoch 5 |  iter 341 / 351 | time 140[s] | loss 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "\u001b[92m☑\u001b[0m 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "\u001b[92m☑\u001b[0m 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "\u001b[92m☑\u001b[0m 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "\u001b[92m☑\u001b[0m 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "\u001b[92m☑\u001b[0m 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "\u001b[92m☑\u001b[0m 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "\u001b[92m☑\u001b[0m 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "\u001b[92m☑\u001b[0m 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "\u001b[92m☑\u001b[0m 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "\u001b[92m☑\u001b[0m 2016-11-06\n",
      "---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val acc 99.920%\n",
      "| epoch 6 |  iter 1 / 351 | time 0[s] | loss 0.00\n",
      "| epoch 6 |  iter 21 / 351 | time 8[s] | loss 0.00\n",
      "| epoch 6 |  iter 41 / 351 | time 17[s] | loss 0.00\n",
      "| epoch 6 |  iter 61 / 351 | time 27[s] | loss 0.00\n",
      "| epoch 6 |  iter 81 / 351 | time 35[s] | loss 0.00\n",
      "| epoch 6 |  iter 101 / 351 | time 44[s] | loss 0.00\n",
      "| epoch 6 |  iter 121 / 351 | time 52[s] | loss 0.00\n",
      "| epoch 6 |  iter 141 / 351 | time 60[s] | loss 0.00\n",
      "| epoch 6 |  iter 161 / 351 | time 69[s] | loss 0.00\n",
      "| epoch 6 |  iter 181 / 351 | time 77[s] | loss 0.00\n",
      "| epoch 6 |  iter 201 / 351 | time 85[s] | loss 0.00\n",
      "| epoch 6 |  iter 221 / 351 | time 94[s] | loss 0.00\n",
      "| epoch 6 |  iter 241 / 351 | time 102[s] | loss 0.00\n",
      "| epoch 6 |  iter 261 / 351 | time 110[s] | loss 0.00\n",
      "| epoch 6 |  iter 281 / 351 | time 119[s] | loss 0.00\n",
      "| epoch 6 |  iter 301 / 351 | time 127[s] | loss 0.00\n",
      "| epoch 6 |  iter 321 / 351 | time 136[s] | loss 0.00\n",
      "| epoch 6 |  iter 341 / 351 | time 144[s] | loss 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "\u001b[92m☑\u001b[0m 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "\u001b[92m☑\u001b[0m 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "\u001b[92m☑\u001b[0m 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "\u001b[92m☑\u001b[0m 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "\u001b[92m☑\u001b[0m 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "\u001b[92m☑\u001b[0m 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "\u001b[92m☑\u001b[0m 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "\u001b[92m☑\u001b[0m 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "\u001b[92m☑\u001b[0m 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "\u001b[92m☑\u001b[0m 2016-11-06\n",
      "---\n",
      "val acc 99.920%\n",
      "| epoch 7 |  iter 1 / 351 | time 0[s] | loss 0.00\n",
      "| epoch 7 |  iter 21 / 351 | time 8[s] | loss 0.00\n",
      "| epoch 7 |  iter 41 / 351 | time 16[s] | loss 0.00\n",
      "| epoch 7 |  iter 61 / 351 | time 24[s] | loss 0.00\n",
      "| epoch 7 |  iter 81 / 351 | time 33[s] | loss 0.00\n",
      "| epoch 7 |  iter 101 / 351 | time 41[s] | loss 0.00\n",
      "| epoch 7 |  iter 121 / 351 | time 49[s] | loss 0.00\n",
      "| epoch 7 |  iter 141 / 351 | time 58[s] | loss 0.00\n",
      "| epoch 7 |  iter 161 / 351 | time 66[s] | loss 0.00\n",
      "| epoch 7 |  iter 181 / 351 | time 74[s] | loss 0.00\n",
      "| epoch 7 |  iter 201 / 351 | time 83[s] | loss 0.00\n",
      "| epoch 7 |  iter 221 / 351 | time 91[s] | loss 0.00\n",
      "| epoch 7 |  iter 241 / 351 | time 100[s] | loss 0.00\n",
      "| epoch 7 |  iter 261 / 351 | time 108[s] | loss 0.00\n",
      "| epoch 7 |  iter 281 / 351 | time 116[s] | loss 0.00\n",
      "| epoch 7 |  iter 301 / 351 | time 125[s] | loss 0.00\n",
      "| epoch 7 |  iter 321 / 351 | time 133[s] | loss 0.00\n",
      "| epoch 7 |  iter 341 / 351 | time 141[s] | loss 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "\u001b[92m☑\u001b[0m 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "\u001b[92m☑\u001b[0m 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "\u001b[92m☑\u001b[0m 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "\u001b[92m☑\u001b[0m 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "\u001b[92m☑\u001b[0m 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "\u001b[92m☑\u001b[0m 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "\u001b[92m☑\u001b[0m 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "\u001b[92m☑\u001b[0m 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "\u001b[92m☑\u001b[0m 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "\u001b[92m☑\u001b[0m 2016-11-06\n",
      "---\n",
      "val acc 99.920%\n",
      "| epoch 8 |  iter 1 / 351 | time 0[s] | loss 0.00\n",
      "| epoch 8 |  iter 21 / 351 | time 9[s] | loss 0.00\n",
      "| epoch 8 |  iter 41 / 351 | time 17[s] | loss 0.00\n",
      "| epoch 8 |  iter 61 / 351 | time 25[s] | loss 0.00\n",
      "| epoch 8 |  iter 81 / 351 | time 34[s] | loss 0.00\n",
      "| epoch 8 |  iter 101 / 351 | time 42[s] | loss 0.00\n",
      "| epoch 8 |  iter 121 / 351 | time 50[s] | loss 0.00\n",
      "| epoch 8 |  iter 141 / 351 | time 59[s] | loss 0.00\n",
      "| epoch 8 |  iter 161 / 351 | time 67[s] | loss 0.00\n",
      "| epoch 8 |  iter 181 / 351 | time 75[s] | loss 0.00\n",
      "| epoch 8 |  iter 201 / 351 | time 84[s] | loss 0.00\n",
      "| epoch 8 |  iter 221 / 351 | time 92[s] | loss 0.00\n",
      "| epoch 8 |  iter 241 / 351 | time 100[s] | loss 0.00\n",
      "| epoch 8 |  iter 261 / 351 | time 109[s] | loss 0.00\n",
      "| epoch 8 |  iter 281 / 351 | time 117[s] | loss 0.00\n",
      "| epoch 8 |  iter 301 / 351 | time 125[s] | loss 0.00\n",
      "| epoch 8 |  iter 321 / 351 | time 134[s] | loss 0.00\n",
      "| epoch 8 |  iter 341 / 351 | time 142[s] | loss 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "\u001b[92m☑\u001b[0m 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "\u001b[92m☑\u001b[0m 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "\u001b[92m☑\u001b[0m 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "\u001b[92m☑\u001b[0m 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "\u001b[92m☑\u001b[0m 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "\u001b[92m☑\u001b[0m 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "\u001b[92m☑\u001b[0m 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "\u001b[92m☑\u001b[0m 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "\u001b[92m☑\u001b[0m 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "\u001b[92m☑\u001b[0m 2016-11-06\n",
      "---\n",
      "val acc 99.960%\n",
      "| epoch 9 |  iter 1 / 351 | time 0[s] | loss 0.00\n",
      "| epoch 9 |  iter 21 / 351 | time 8[s] | loss 0.00\n",
      "| epoch 9 |  iter 41 / 351 | time 16[s] | loss 0.00\n",
      "| epoch 9 |  iter 61 / 351 | time 25[s] | loss 0.00\n",
      "| epoch 9 |  iter 81 / 351 | time 33[s] | loss 0.00\n",
      "| epoch 9 |  iter 101 / 351 | time 41[s] | loss 0.00\n",
      "| epoch 9 |  iter 121 / 351 | time 49[s] | loss 0.00\n",
      "| epoch 9 |  iter 141 / 351 | time 58[s] | loss 0.00\n",
      "| epoch 9 |  iter 161 / 351 | time 66[s] | loss 0.00\n",
      "| epoch 9 |  iter 181 / 351 | time 74[s] | loss 0.00\n",
      "| epoch 9 |  iter 201 / 351 | time 82[s] | loss 0.00\n",
      "| epoch 9 |  iter 221 / 351 | time 90[s] | loss 0.00\n",
      "| epoch 9 |  iter 241 / 351 | time 99[s] | loss 0.00\n",
      "| epoch 9 |  iter 261 / 351 | time 107[s] | loss 0.00\n",
      "| epoch 9 |  iter 281 / 351 | time 115[s] | loss 0.00\n",
      "| epoch 9 |  iter 301 / 351 | time 123[s] | loss 0.00\n",
      "| epoch 9 |  iter 321 / 351 | time 132[s] | loss 0.00\n",
      "| epoch 9 |  iter 341 / 351 | time 140[s] | loss 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "\u001b[92m☑\u001b[0m 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "\u001b[92m☑\u001b[0m 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "\u001b[92m☑\u001b[0m 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "\u001b[92m☑\u001b[0m 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "\u001b[92m☑\u001b[0m 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "\u001b[92m☑\u001b[0m 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "\u001b[92m☑\u001b[0m 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "\u001b[92m☑\u001b[0m 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "\u001b[92m☑\u001b[0m 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "\u001b[92m☑\u001b[0m 2016-11-06\n",
      "---\n",
      "val acc 99.960%\n",
      "| epoch 10 |  iter 1 / 351 | time 0[s] | loss 0.00\n",
      "| epoch 10 |  iter 21 / 351 | time 8[s] | loss 0.00\n",
      "| epoch 10 |  iter 41 / 351 | time 17[s] | loss 0.00\n",
      "| epoch 10 |  iter 61 / 351 | time 25[s] | loss 0.00\n",
      "| epoch 10 |  iter 81 / 351 | time 33[s] | loss 0.00\n",
      "| epoch 10 |  iter 101 / 351 | time 41[s] | loss 0.00\n",
      "| epoch 10 |  iter 121 / 351 | time 50[s] | loss 0.00\n",
      "| epoch 10 |  iter 141 / 351 | time 58[s] | loss 0.00\n",
      "| epoch 10 |  iter 161 / 351 | time 66[s] | loss 0.00\n",
      "| epoch 10 |  iter 181 / 351 | time 75[s] | loss 0.00\n",
      "| epoch 10 |  iter 201 / 351 | time 83[s] | loss 0.00\n",
      "| epoch 10 |  iter 221 / 351 | time 91[s] | loss 0.00\n",
      "| epoch 10 |  iter 241 / 351 | time 99[s] | loss 0.00\n",
      "| epoch 10 |  iter 261 / 351 | time 107[s] | loss 0.00\n",
      "| epoch 10 |  iter 281 / 351 | time 116[s] | loss 0.00\n",
      "| epoch 10 |  iter 301 / 351 | time 124[s] | loss 0.00\n",
      "| epoch 10 |  iter 321 / 351 | time 132[s] | loss 0.00\n",
      "| epoch 10 |  iter 341 / 351 | time 140[s] | loss 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "\u001b[92m☑\u001b[0m 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "\u001b[92m☑\u001b[0m 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "\u001b[92m☑\u001b[0m 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "\u001b[92m☑\u001b[0m 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "\u001b[92m☑\u001b[0m 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "\u001b[92m☑\u001b[0m 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "\u001b[92m☑\u001b[0m 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "\u001b[92m☑\u001b[0m 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "\u001b[92m☑\u001b[0m 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "\u001b[92m☑\u001b[0m 2016-11-06\n",
      "---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val acc 99.960%\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAdx0lEQVR4nO3de3hV9b3n8feXXCBcAxJuAQUVuQgIGm+19nhpy80K9elMta2n9ZzWcUZ7Oc8ZR21npj3tPE+dx3N6Ts/UUw7j2NaePrVnqiegRKOi4m2sYAmEi8GIWkgCSUCQS+75zh97BTYhgZ2Qxdp7r8/redJmrb323t9sw/5kf9dv/X7m7oiISHwNiroAERGJloJARCTmFAQiIjGnIBARiTkFgYhIzOVGXUBfjR071qdOnRp1GSIiGeXtt99udPeinm7LuCCYOnUqGzZsiLoMEZGMYmYf9nabWkMiIjGnIBARiTkFgYhIzCkIRERiTkEgIhJzoY0aMrNHgZuAenef08PtBvwUWAIcBb7m7n8Mqx45rnRjDQ+VV1F7oIlJhQXcu3AGyxcUqw7VkRY1qI6zX0eYw0d/CfwMeKyX2xcD04OvK4GfB/8vISrdWMMDT1bS1NYBQM2BJh54shLgrP6Cq470qyMdalAd0dQRWhC4+ytmNvUUhywDHvPEPNhvmlmhmU1097qwahJ4qLzq2C9Ul6a2Dn749DaG5uectTp++PS2XusYktdVx4lTpCfPmO697E/c5j3e1n3CdXfnb57a2mMdf/PUVsxS+EEGSDrUcaoaHD/2Wroffy27prH3Y/9z/PU/8bjjtx3//sQbPfj2J8/v6LGO76/ewoGjrWf6Y6bs719I7zoeKq8asCCwMNcjCILg6V5aQ08DD7r7a8H2WuA+dz/pajEzuxO4E+Dcc8+97MMPe70uQk5j2v1rTnpDFJHMY8D7Dy5N/Xizt929pKfboryyuKe/c3p8j3L3lcBKgJKSEr2PnYFJhQXUHGg6aX/RiMH84muXn7U67vjlehoOtfRYx6/uuOLYdve/hpO3LelX6KTjerlP91+7L/3vN6nvoY5xIwbz2zuv6q38AXfbyujr6K2G8SMH87s7rwaOv5aG9fjfxoKdlrwv2Dp+3+MH9HTbTf/rNeoONp9Ux8RRQyj71rX9+dH6Zck/vprWdUwqLBiw54gyCHYDU5K2JwO1EdUSG/cunMF//r+baO88nqcFeTl8b8ks5hSPOmt1fG/JrBP6nsl1zJ408qzV8d1e6vjukllcUDQ8VnX0VsMDi2cxdeyws1IDwH2LZvZYx32LZjJ6WL7qCOq4d+GMAXuOKINgNXCPmT1O4iTxQZ0fCN+y+ZN48Jnt7D/SRltHZ2QjIbqeL+oRGaojvWpQHdHUEdo5AjP7LXAdMBbYC3wfyANw9xXB8NGfAYtIDB+9o6fzA92VlJS4Jp3rv+11H7P4p6/yo2UXc/vVU6MuR0TOkkjOEbj7bae53YG7w3p+6VlpRQ25g4yl8yZFXYqIpAldWRwjnZ3OUxW1XDt9LGPOYo9TRNKbgiBG1n+wn9qDzZFcGSki6UtBECOlFbUU5OXw6Vnjoy5FRNKIgiAmWts7Kaus47MXj2fY4IxbmE5EQqQgiIl1Oxo42NTG8vlqC4nIiRQEMbGqooYxw/L55PSxUZciImlGQRADh1vaeWH7XpbOnUhejv6Ti8iJ9K4QA+Vb9tDc1smy+bp2QEROpiCIgVWbapk8uoDLzhsddSkikoYUBFmu4VALr73bwLL5k47NDCkikkxBkOXWbK6l02GZRguJSC8UBFmutKKWWRNHctH4EVGXIiJpSkGQxT5oPELFrgM6SSwip6QgyGKrN9ViBjdfoiAQkd4pCLKUu1NaUcMVU8cM6JJ2IpJ9FARZamvtx+xsOKKTxCJyWgqCLFW6sYa8HGPJ3AlRlyIiaU5BkIU6Op3Vm2q5bsY4CodqARoROTUFQRb6w8591B9q0WghEUmJgiALlVbUMCxfC9CISGoUBFmmua2DZ7bsYeGcCQzJy4m6HBHJAAqCLPNyVT2Hmtu1AI2IpExBkGVKN9YydvhgPnHBOVGXIiIZQkGQRQ42tfFiVT03zZtIrhagEZEU6d0ii5Rv2UNreyfLF6gtJCKpUxBkkVWbajjvnKFcMnlU1KWISAZREGSJvR8388Z7+1g2v1gL0IhInygIssRTm2pxRxeRiUifKQiyxKqKWuYWj+KCouFRlyIiGUZBkAXeazhMZc1BfRoQkX5REGSBVRWJBWg+pwVoRKQfQg0CM1tkZlVmVm1m9/dw+ygze8rMNpnZVjO7I8x6spG7s6qihk9ccA7jRw6JuhwRyUChBYGZ5QAPA4uB2cBtZja722F3A9vc/RLgOuDvzEzzJvdBxa4DfLjvqBagEZF+C/MTwRVAtbvvdPdW4HFgWbdjHBhhifGOw4H9QHuINWWdVRW15OcOYtEcLUAjIv0TZhAUA7uStncH+5L9DJgF1AKVwLfdvbP7A5nZnWa2wcw2NDQ0hFVvxmnv6OTpzbXcOHMcI4fkRV2OiGSoMIOgp6uavNv2QqACmATMB35mZiNPupP7SncvcfeSoqKiga4zY73x3j4aD7dqtJCInJEwg2A3MCVpezKJv/yT3QE86QnVwPvAzBBryiqlFTWMGJLLdTPGRV2KiGSwMINgPTDdzKYFJ4BvBVZ3O+ZPwI0AZjYemAHsDLGmrNHU2kH5lj0smTNRC9CIyBnJDeuB3b3dzO4ByoEc4FF332pmdwW3rwB+BPzSzCpJtJLuc/fGsGrKJmvf2cuR1g61hUTkjIUWBADuXgaUddu3Iun7WuCzYdaQrUo31jJ+5GCuPF8L0IjImdGVxRnowNFW1u2o5+ZLJpEzSDONisiZURBkoLLKPbR1uC4iE5EBoSDIQKUVNVxQNIyLJ5000lZEpM8UBBmm9kATb72/XwvQiMiAURBkmNWbEpdiaLSQiAwUBUGGWVVRy4JzCznvnGFRlyIiWUJBkEF27D3E9rqPWaZ1B0RkACkIMkjpxhpyBhlL5ykIRGTgKAgyRGIBmlquuXAsRSMGR12OiGQRBUGGePvDj6g50MRynSQWkQGmIMgQqypqGZI3iM9erAVoRGRgKQgyQFtHJ2sq6/j0rPEMHxzq9FAiEkMKggzw6rsN7D/SynJNKSEiIVAQZIBVFbUUDs3jUxdpdTYRGXgKgjR3pKWd57buZcncieTn6j+XiAw8vbOkuRe276WprUNtIREJjYIgzZVurGHSqCGUnDc66lJEJEspCNLYvsMtvPJuIzfPL2aQFqARkZAoCNJYWWUdHZ2umUZFJFQKgjRWWlHLjPEjmDVRC9CISHgUBGlq1/6jvP3hRyxboE8DIhIuBUGa6lqA5mZNOS0iIVMQpCF3p3RjDZdPHc3k0UOjLkdEspyCIA1trzvEu/WHuVnXDojIWaAgSEOrKmrIHWQsnTsx6lJEJAYUBGmms9NZvamWP7uoiDHD8qMuR0RiQEGQZt76YD91B5u5WdcOiMhZoiBIM6sqahian8NnZo+PuhQRiQkFQRppae+grHIPCy+ewNB8LUAjImeHgiCNrKtq4GBTm9pCInJWhRoEZrbIzKrMrNrM7u/lmOvMrMLMtprZujDrSXerNtVyzrB8Pnnh2KhLEZEYCa3/YGY5wMPAZ4DdwHozW+3u25KOKQT+CVjk7n8ys3Fh1ZPuDjW38cK2vXzx8ink5eiDmoicPWG+41wBVLv7TndvBR4HlnU75kvAk+7+JwB3rw+xnrRWvnUvLe2dLNNFZCJylqUUBGb2hJktNbO+BEcxsCtpe3ewL9lFwGgze9nM3jazP+/l+e80sw1mtqGhoaEPJWSOVRU1TBlTwKXnFkZdiojETKpv7D8n8df7u2b2oJnNTOE+Pa2k4t22c4HLgKXAQuC/mdlFJ93JfaW7l7h7SVFR9i3gXn+omderG1l2STFmWoBGRM6ulILA3V9w9y8DlwIfAM+b2RtmdoeZ5fVyt93AlKTtyUBtD8c86+5H3L0ReAW4pC8/QDZYs7mOToflmnJaRCKQcqvHzM4BvgZ8HdgI/JREMDzfy13WA9PNbJqZ5QO3Aqu7HbMKuNbMcs1sKHAlsL1PP0EWKK2oZfbEkVw4bkTUpYhIDKU0asjMngRmAr8GPufudcFNvzOzDT3dx93bzeweoBzIAR51961mdldw+wp3325mzwKbgU7gEXffcmY/UmZ5v/EIm3Yd4LtLUum2iYgMvFSHj/7M3V/s6QZ3L+ntTu5eBpR127ei2/ZDwEMp1pF1VlfUYgaf0wI0IhKRVFtDs4Ix/wCY2Wgz+0/hlBQf7s6qihqunDaGiaMKoi5HRGIq1SD4hrsf6Npw94+Ab4RSUYxsqfmYnY1HWK5rB0QkQqkGwSBLGtcYXDWsyfLPUGlFDfk5g1g8RwvQiEh0Uj1HUA78q5mtIHEtwF3As6FVFQMdnc5Tm2q5bkYRo4b2NgJXRCR8qQbBfcB/AP4jiQvFngMeCauoOHhz5z7qD7WwfIHaQiISrZSCwN07SVxd/PNwy4mP0o01DB+cyw0zYzvPnoikiVSvI5gO/BiYDQzp2u/u54dUV1Zrbuvg2S17WDRnAkPycqIuR0RiLtWTxb8g8WmgHbgeeIzExWXSDy+9U8+hlnaWaQEaEUkDqZ4jKHD3tWZm7v4h8AMzexX4foi1ZZ3SjTU8VF5FzYEmBhk0fNwSdUkiIikHQXMwBfW7wbQRNYCa231QurGGB56spKmtA4BOh++VbmHQINMJYxGJVKqtoe8AQ4FvkZg2+ivAV0OqKSs9VF51LAS6NLV18FB5VUQViYgknPYTQXDx2L9393uBw8AdoVeVhWoPNPVpv4jI2XLaTwTu3gFcZlox5YxMKux5LqHe9ouInC2ptoY2AqvM7HYzu6XrK8zCss29C2eQl3Nilhbk5XDvwhkRVSQikpBqEIwB9gE3AJ8Lvm4Kq6hstHxBMecXDSdnkGFAcWEBP75lrk4Ui0jkUr2yWOcFztCBo628V3+Yb1x7Pvcv1iI0IpI+Ur2y+BecvPA87v4XA15Rlnpu617aO52lczXTqIikl1SvI3g66fshwOc5eSF6OYU1lXVMGVPAnOKRUZciInKCVFtDTyRvm9lvgRdCqSgLHTjayuvVjfzltdPQ4CsRSTepnizubjpw7kAWks2e26a2kIikr1TPERzixHMEe0isUSApKKusY/LoAuYWj4q6FBGRk6TaGhoRdiHZ6uDRNl6vbuQvrlFbSETSU0qtITP7vJmNStouNLPloVWVRZ7btoe2DmeJ2kIikqZSPUfwfXc/2LXh7gfQFNQpKauso7iwgHmT1RYSkfSUahD0dFyqQ09j62BTG69VN7Jk7gS1hUQkbaUaBBvM7CdmdoGZnW9mfw+8HWZh2eD5bXvVFhKRtJdqEHwTaAV+B/wr0ATcHVZR2aKrLTR/SmHUpYiI9CrVUUNHgPtDriWrHGxq49V3G/jq1VPVFhKRtJbqqKHnzawwaXu0mZWHVlUWeKGrLTRPbSERSW+ptobGBiOFAHD3j9Caxaf0zJY6Jo0awgK1hUQkzaUaBJ1mdmxKCTObSg+zkUrCx81tvLKjkcVzJ6otJCJpL9Ug+B7wmpn92sx+DawDHjjdncxskZlVmVm1mfV6jsHMLjezDjP7Qor1pLW12/fS2tGp0UIikhFSCgJ3fxYoAapIjBz6axIjh3oVLHr/MLAYmA3cZmazeznufwJZc85hzeY9TFRbSEQyRKqTzn0d+DYwGagArgL+H4mlK3tzBVDt7juDx3gcWAZs63bcN4EngMv7Uni6OtTcxivvNvCVK89j0CC1hUQk/aXaGvo2iTfqD939emAB0HCa+xQDu5K2dwf7jjGzYhKL3Kw41QOZ2Z1mtsHMNjQ0nO5po7V2ez2t7Z0snTch6lJERFKSahA0u3szgJkNdvd3gBmnuU9Pfw53P8H8D8B97t5xqgdy95XuXuLuJUVFRSmWHI01lXVMGDmEBVNGR12KiEhKUp0vaHdwHUEp8LyZfcTpl6rcDUxJ2p7cw31KgMeDkTVjgSVm1u7upSnWlVYONbexbkcDX77yXLWFRCRjpHpl8eeDb39gZi8Bo4BnT3O39cB0M5sG1AC3Al/q9rjTur43s18CT2dqCAC8+E7QFtJoIRHJIH2eQdTd16V4XLuZ3UNiNFAO8Ki7bzWzu4LbT3leIBOt2ZxoC116rtpCIpI5Qp1K2t3LgLJu+3oMAHf/Wpi1hO1wSzsv72jgS1eoLSQimaW/i9dLN2u37w1GC6ktJCKZRUEwQMoq6xg3YjCXqS0kIhlGQTAAjrS083JVA4vnTFBbSEQyjoJgAKx9p56Wds0tJCKZSUEwAMo211E0YjAlU8dEXYqISJ8pCM7QkZZ2XqqqZ/GcCeSoLSQiGUhBcIZeVFtIRDKcguAMlVXWMXb4YC5XW0hEMpSC4AwcbVVbSEQyn4LgDLz0TgPNbWoLiUhmUxCcga620BXT1BYSkcylIOinptYOXnynnkVzxqstJCIZTUHQTy9V1dPU1qG2kIhkPAVBP62prGPs8HyunHZO1KWIiJwRBUE/NLV28OL2ehZerNFCIpL5FAT98HLQFtJKZCKSDRQE/bCmso5zhuVrtJCIZAUFQR81tyVGCy2cM4HcHL18IpL59E7WRy9X1XO0tYMlc9QWEpHsoCDoozWVexgzLJ+rzldbSESyg4KgD5rbOli7fS8LLx6vtpCIZA29m/XBy1UNibaQRguJSBZREPRBWWUdo4fmcfX5uohMRLKHgiBFx9tCGi0kItlF72gpWrejgSNqC4lIFlIQpKisso7CoXlcfYHaQiKSXRQEKUi0hepZOHsCeWoLiUiW0btaCl7Z0cDhlnaWzFNbSESyj4IgBV1toU+oLSQiWUhBcBot7R28sL2ez84er7aQiGSlUN/ZzGyRmVWZWbWZ3d/D7V82s83B1xtmdkmY9fTHqzsaE20hjRYSkSwVWhCYWQ7wMLAYmA3cZmazux32PvBn7j4P+BGwMqx6+qusso5RBXlcc+HYqEsREQlFmJ8IrgCq3X2nu7cCjwPLkg9w9zfc/aNg801gcoj19FlLewfPb9urtpCIZLUw392KgV1J27uDfb35S+CZnm4wszvNbIOZbWhoaBjAEk/ttXcbOaTRQiKS5cIMgp4W8/UeDzS7nkQQ3NfT7e6+0t1L3L2kqKhoAEs8tTWVdYwckss1F6gtJCLZKzfEx94NTEnangzUdj/IzOYBjwCL3X1fiPX0yfG20ATyc9UWEpHsFeY73HpguplNM7N84FZgdfIBZnYu8CRwu7vvCLGWPnu9upFDze0snTch6lJEREIV2icCd283s3uAciAHeNTdt5rZXcHtK4D/DpwD/JOZAbS7e0lYNfXFms17GDEkl09eePZaUSIiUQizNYS7lwFl3fatSPr+68DXw6yhP1rbO3l+2x4+M3u82kIikvX0LteD16sb+bi5naW6iExEYkBB0IM1lXWMGJzLJ6drtJCIZD8FQTet7Z08tzXRFhqcmxN1OSIioVMQdPP6e4m2kOYWEpG4UBB0U7Y50Ra69iK1hUQkHhQESdo6Onlu214+rbaQiMSIgiDJ69WNHGxqU1tIRGJFQZCkrLKO4YNzuVajhUQkRhQEgWNtoVnjGJKntpCIxIeCIPDGe/s4cFRtIRGJHwVB4JmgLfSpizS3kIjEi4KARFuofOseblRbSERiSEEAvLlzHx8dbWPxHLWFRCR+FAQkRgsNy8/huhlqC4lI/MQ+CNo7OinfupcbZo1XW0hEYin2QfDmzv3sP9LK0rlaiUxE4in2QbCmso6h+TlcN2Nc1KWIiEQi1kHQHowWumGmRguJSHzFOgj+8H5XW0ijhUQkvmIdBGsq6yjIU1tIROIttkHQ3tFJ+ZY93DBrHAX5aguJSHzFNgjeen8/+9QWEhGJbxB0tYWuV1tIRGIulkHQ0enHRgupLSQicRfLIPjD+/toPNyqKadFRIhpEJRV1jEkbxDXz9TcQiIisQuCjk7n2S17uWHmOIbm50ZdjohI5GIXBG+9v5/Gwy1qC4mIBGIXBMfaQhotJCICxCwIOjqdZ7bs4bqLxjFssNpCIiIQsyDY8EHQFpqntpCISJdQg8DMFplZlZlVm9n9PdxuZvaPwe2bzezSMOoo3VjDNQ++yBdXvglAU0t7GE8jIpKRQgsCM8sBHgYWA7OB28xsdrfDFgPTg687gZ8PdB2lG2t44MlKag40Hdv3g6e2UbqxZqCfSkQkI4X5ieAKoNrdd7p7K/A4sKzbMcuAxzzhTaDQzAa0b/NQeRVNbR0n7Gtq6+Ch8qqBfBoRkYwVZhAUA7uStncH+/p6DGZ2p5ltMLMNDQ0NfSqiNumTQCr7RUTiJswgsB72eT+Owd1XunuJu5cUFfXtauBJhQV92i8iEjdhBsFuYErS9mSgth/HnJF7F86goNsylAV5Ody7cMZAPo2ISMYKMwjWA9PNbJqZ5QO3Aqu7HbMa+PNg9NBVwEF3rxvIIpYvKObHt8yluLAAA4oLC/jxLXNZvuCkDpSISCyFdlWVu7eb2T1AOZADPOruW83sruD2FUAZsASoBo4Cd4RRy/IFxXrjFxHpRaiX17p7GYk3++R9K5K+d+DuMGsQEZFTi9WVxSIicjIFgYhIzCkIRERiTkEgIhJzljhfmznMrAH4sJ93Hws0DmA5mU6vx4n0ehyn1+JE2fB6nOfuPV6Rm3FBcCbMbIO7l0RdR7rQ63EivR7H6bU4Uba/HmoNiYjEnIJARCTm4hYEK6MuIM3o9TiRXo/j9FqcKKtfj1idIxARkZPF7ROBiIh0oyAQEYm52ASBmS0ysyozqzaz+6OuJ0pmNsXMXjKz7Wa21cy+HXVNUTOzHDPbaGZPR11L1Mys0Mx+b2bvBL8jV0ddU1TM7K+CfyNbzOy3ZjYk6prCEIsgMLMc4GFgMTAbuM3MZkdbVaTagb9291nAVcDdMX89AL4NbI+6iDTxU+BZd58JXEJMXxczKwa+BZS4+xwS0+nfGm1V4YhFEABXANXuvtPdW4HHgWUR1xQZd69z9z8G3x8i8Q89tgs2mNlkYCnwSNS1RM3MRgKfAv4PgLu3uvuBSIuKVi5QYGa5wFAGeAXFdBGXICgGdiVt7ybGb3zJzGwqsAD4Q8SlROkfgP8CdEZcRzo4H2gAfhG0yh4xs2FRFxUFd68B/hb4E1BHYgXF56KtKhxxCQLrYV/sx82a2XDgCeA77v5x1PVEwcxuAurd/e2oa0kTucClwM/dfQFwBIjlOTUzG02iczANmAQMM7OvRFtVOOISBLuBKUnbk8nSj3ipMrM8EiHwG3d/Mup6InQNcLOZfUCiZXiDmf1LtCVFajew2927PiH+nkQwxNGngffdvcHd24AngU9EXFMo4hIE64HpZjbNzPJJnPBZHXFNkTEzI9ED3u7uP4m6nii5+wPuPtndp5L4vXjR3bPyr75UuPseYJeZzQh23Qhsi7CkKP0JuMrMhgb/Zm4kS0+ch7pmcbpw93YzuwcoJ3Hm/1F33xpxWVG6BrgdqDSzimDfd4M1pkW+Cfwm+KNpJ3BHxPVEwt3/YGa/B/5IYqTdRrJ0qglNMSEiEnNxaQ2JiEgvFAQiIjGnIBARiTkFgYhIzCkIRERiTkEgEjIzu06zmko6UxCIiMScgkAkYGZfMbO3zKzCzP45WKPgsJn9nZn90czWmllRcOx8M3vTzDab2b8F89JgZhea2Qtmtim4zwXBww9PmuP/N8GVqpjZg2a2LXicv43oR5eYUxCIAGY2C/gicI27zwc6gC8Dw4A/uvulwDrg+8FdHgPuc/d5QGXS/t8AD7v7JSTmpakL9i8AvkNiPYzzgWvMbAzweeDi4HH+R5g/o0hvFAQiCTcClwHrg2k3biTxht0J/C445l+AT5rZKKDQ3dcF+38FfMrMRgDF7v5vAO7e7O5Hg2Pecvfd7t4JVABTgY+BZuARM7sF6DpW5KxSEIgkGPArd58ffM1w9x/0cNyp5mTpabrzLi1J33cAue7eTmLRpCeA5cCzfStZZGAoCEQS1gJfMLNxAGY2xszOI/Fv5AvBMV8CXnP3g8BHZnZtsP92YF2wpsNuM1sePMZgMxva2xMG60GMCib7+w4wf8B/KpEUxGL2UZHTcfdtZvZfgefMbBDQBtxNYmGWi83sbeAgifMIAF8FVgRv9MkzdN4O/LOZ/TB4jH93iqcdAawKFkQ34K8G+McSSYlmHxU5BTM77O7Do65DJExqDYmIxJw+EYiIxJw+EYiIxJyCQEQk5hQEIiIxpyAQEYk5BYGISMz9f7w2QQ/6SegoAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset import sequence\n",
    "from common.optimizer import Adam\n",
    "from common.trainer import Trainer\n",
    "from common.util import eval_seq2seq\n",
    "from ch08.attention_seq2seq import AttentionSeq2seq\n",
    "from ch07.seq2seq import Seq2seq\n",
    "from ch07.peeky_seq2seq import PeekySeq2seq\n",
    "\n",
    "# 读入数据\n",
    "(x_train, t_train), (x_test, t_test) = sequence.load_data('date.txt')\n",
    "char_to_id, id_to_char = sequence.get_vocab()\n",
    "\n",
    "# 反转输入语句\n",
    "x_train, x_test = x_train[:, ::-1], x_test[:, ::-1]\n",
    "\n",
    "# 设定超参数\n",
    "vocab_size = len(char_to_id)\n",
    "wordvec_size = 16\n",
    "hidden_size = 256\n",
    "batch_size = 128\n",
    "max_epoch = 10\n",
    "max_grad = 5.0\n",
    "\n",
    "model = AttentionSeq2seq(vocab_size, wordvec_size, hidden_size)\n",
    "# model = Seq2seq(vocab_size, wordvec_size, hidden_size)\n",
    "# model = PeekySeq2seq(vocab_size, wordvec_size, hidden_size)\n",
    "\n",
    "optimizer = Adam()\n",
    "trainer = Trainer(model, optimizer)\n",
    "\n",
    "acc_list = []\n",
    "for epoch in range(max_epoch):\n",
    "    trainer.fit(x_train, t_train, max_epoch=1,\n",
    "                batch_size=batch_size, max_grad=max_grad)\n",
    "\n",
    "    correct_num = 0\n",
    "    for i in range(len(x_test)):\n",
    "        question, correct = x_test[[i]], t_test[[i]]\n",
    "        verbose = i < 10\n",
    "        correct_num += eval_seq2seq(model, question, correct,\n",
    "                                    id_to_char, verbose, is_reverse=True)\n",
    "\n",
    "    acc = float(correct_num) / len(x_test)\n",
    "    acc_list.append(acc)\n",
    "    print('val acc %.3f%%' % (acc * 100))\n",
    "\n",
    "\n",
    "model.save_params()\n",
    "\n",
    "# 绘制图形\n",
    "x = np.arange(len(acc_list))\n",
    "plt.plot(x, acc_list, marker='o')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('accuracy')\n",
    "plt.ylim(-0.05, 1.05)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc692d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
